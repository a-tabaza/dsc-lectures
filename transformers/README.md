# Transformers: A Very Gentle Introduction

**Synopsis:** Introduction to Deep Learning from ANNs to Transformers including Attention, Self-attention, Encoders and Decoders, and Transformers in detail. High level, minimal math, meant to introduce the concepts behind most of these breakthroughs with ease and encourage self learning.

## Resources to Dig Deeper

### More Key Applications (Advanced)

Links to papers about state of the art models in more advanced areas of deep learning:

- [Multivariate Timeseries Forecasting: Informer](https://arxiv.org/abs/2012.07436)
- [Automatic Speech Recognition: Conformer](https://arxiv.org/abs/2005.08100)
- [Recommender Systems: BERT4Rec](https://arxiv.org/abs/1904.06690)
- [Reinforcement Learning: Decision Transformer](https://arxiv.org/abs/2106.01345)

### Textbooks

- [Understanding Deep Learning by J.D. Prince (Beginner)](https://udlbook.github.io/udlbook/)
- [Multimodal Deep Learning (Advanced)](https://arxiv.org/abs/2301.04856)

### Domain Specific Models

- [BioMedLM](https://huggingface.co/stanford-crfm/BioMedLM)
- [BioGPT](https://github.com/microsoft/BioGPT)
- [BloombergGPT](https://arxiv.org/pdf/2303.17564v1.pdf)

### Coding Resources

- [PyTorch Fundamentals from Microsoft (Beginner)](https://learn.microsoft.com/en-us/training/paths/pytorch-fundamentals/)
- [Understanding and Coding the Self-Attention Mechanism of Large Language Models From Scratch  by Sebastian Raschka
(Intermediate)](https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html)
- [Annotated PyTorch Paper Implementations (Advanced)](https://nn.labml.ai/index.html)
- [Illustrated Guides to Transformers (and other Models) by Jay Alammar (This resources was cited in MIT Lectures)](https://jalammar.github.io/)

### Papers

- [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)
- [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)
- [Neural Machine Translation By Jointly Learning To Align And Translate](https://arxiv.org/pdf/1409.0473.pdf)
- [An Image Is Worth 16x16 Words: Transformers For Image Recognition At Scale](https://arxiv.org/pdf/2010.11929.pdf)
- [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)
- [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531)
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)
- [Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)
- [Flamingo: a Visual Language Model for Few-Shot Learning](https://arxiv.org/abs/2204.14198)
- [INVASE: Instance-wise Variable Selection Using Neural Networks](https://openreview.net/pdf?id=BJg_roAcK7)
- [TabNet: Attentive Interpretable Tabular Learning](https://arxiv.org/abs/1908.07442)
- [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)
